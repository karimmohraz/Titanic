{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30cc1a55-3179-41b1-90a8-017f61018131\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(30, 20, 10), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sapdi\n",
    "from sapdi.artifact import Artifact, ArtifactKind, ArtifactFileType\n",
    "from sapdi.internal.datalake.datalake_storage import DatalakeStorage\n",
    "from sapdi.scenario.context import Context\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import uuid\n",
    "from io import BytesIO\n",
    "os.environ[\"DI_SDK_RUNTIME_CONTEXT\"] = \"DI_EXECUTION\"\n",
    "Context.context = None\n",
    "\n",
    "\n",
    "import uuid\n",
    "from sapdi.scenario.scenario import Scenario\n",
    "from sapdi.scenario.context import Context\n",
    "from sapdi.internal.datalake.datalake_storage import DatalakeStorage\n",
    "from sapdi.internal.datalake.datalake_client import DataLakeClient\n",
    "from sapdi.internal.vrep.sharedvolume import SharedVolume\n",
    "from sapdi.internal.di_client import DIClient\n",
    "\n",
    "scenario_id = None # To fill if you execute the pipeline modeller in a scenario context\n",
    "if not scenario_id:\n",
    "    scenario = Scenario(scenario_id=str(uuid.uuid4()), created_by='me', created_at='2020-01-04-15:08:00')\n",
    "else:\n",
    "    scenario = Scenario.get(scenario_id=scenario_id)\n",
    "\n",
    "Context.context = Context(\n",
    "    scenario=scenario,\n",
    "    default_storage=DatalakeStorage(),\n",
    "    runtime_context_env='DI_EXECUTION',\n",
    "    shared_volume=SharedVolume()\n",
    ")\n",
    "\n",
    "DIClient._instance = None\n",
    "DataLakeClient._instance = None\n",
    "\n",
    "class DLPersistor:\n",
    "    @staticmethod\n",
    "    def create_dataset_artifact(file_name, file_directory):\n",
    "        artifact = sapdi.create_artifact(\n",
    "            artifact_kind=ArtifactKind.DATASET,\n",
    "            artifact_name=file_name + \"-\" + str(uuid.uuid4()),\n",
    "            upload_content=os.path.join(file_directory, file_name),\n",
    "            description=file_name)\n",
    "        return artifact\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dataframe_from_dataset_artifact(artifact_id):\n",
    "        artifact = sapdi.get_artifact(artifact_id=artifact_id)\n",
    "        file_handler = artifact.open_file(file_name=artifact.description)\n",
    "        content = file_handler.read()\n",
    "        df = pd.DataFrame([x.split(',') for x in content.decode('utf-8').split('\\n')])\n",
    "        df = pd.DataFrame(df.values[1:], columns=df.iloc[0])\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def create_model_artifact(file_name, file_directory):\n",
    "        artifact = sapdi.create_artifact(\n",
    "            artifact_kind=ArtifactKind.MODEL,\n",
    "            artifact_name=str(uuid.uuid4()),\n",
    "            upload_content=os.path.join(file_directory, file_name),\n",
    "            description=file_name)\n",
    "        return artifact\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model_from_artifact(artifact_id):\n",
    "        artifact = sapdi.get_artifact(artifact_id=artifact_id)\n",
    "        file_handler = artifact.open_file(file_name=artifact.description)\n",
    "        content = file_handler.read()\n",
    "        model = pickle.loads(content)\n",
    "        return model\n",
    "        \n",
    "model1 = pickle.dumps('test_model')\n",
    "# f = open(\"Titanic/model1\", \"wb+\")\n",
    "# f.write(model1)\n",
    "# f.close()\n",
    "\n",
    "dl_persistor = DLPersistor()\n",
    "artifact = dl_persistor.create_model_artifact(\"modelT\", \"Titanic\")\n",
    "print(artifact.artifact_id)\n",
    "art_id = artifact.artifact_id  # '8b283313-3a05-498a-ae20-7a7083bf3320'  # artifact.artifact_id\n",
    "\n",
    "model = dl_persistor.get_model_from_artifact(art_id)\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[1,1,1,0,5,3,3,1,3], [1,1,1,0,5,3,3,1,3], [1,1,1,0,5,3,3,1,3]]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdAt": "2020-01-05T13:01:37Z",
  "createdBy": "admin",
  "description": "create / read artifact",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "modifiedAt": "2020-01-05T13:01:37Z",
  "modifiedBy": "admin",
  "name": "artifact",
  "scenarioId": "0467899d-707f-4c8c-898b-8eb74850c0dc"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
